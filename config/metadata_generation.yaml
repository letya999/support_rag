# Metadata Generation Service Configuration
# ==========================================

metadata_generation:
  # Classification thresholds
  confidence_threshold_high: 0.75  # Use embedding result if >= 0.75
  confidence_threshold_low: 0.70   # Validate intent if < 0.70

  # Context retrieval
  context_examples_count: 3        # Number of example questions per category
  context_chunks_count: 3          # Number of answer chunks per category
  chunk_preview_length: 150        # Characters to extract from answer chunks

  # LLM settings
  llm:
    model: "gpt-3.5-turbo"        # OpenAI model
    temperature: 0.2              # Low temperature for consistency
    max_tokens: 200               # Max tokens per validation

  # Batching
  llm_batch_size: 20              # Items per batch request
  max_parallel_batches: 5         # Max parallel batch requests

  # Cache settings
  cache:
    enabled: true
    ttl_seconds: 3600             # Cache TTL for analysis results
    backend: "redis"              # Cache backend

  # Logging
  logging:
    level: "INFO"
    trace_to_langfuse: true
