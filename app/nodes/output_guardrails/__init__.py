"""
Output Guardrails Node

Validates LLM responses before sending to user:
- Hallucination detection
- Toxicity in responses
- Data leakage prevention
- Relevance checking
- Refusal detection
"""
