# –ü–ª–∞–Ω –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏—è –ø—Ä–æ–±–ª–µ–º –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏

**–î–∞—Ç–∞ –∞–Ω–∞–ª–∏–∑–∞:** 2026-01-13
**–í—Å–µ–≥–æ –Ω–∞–π–¥–µ–Ω–æ –ø—Ä–æ–±–ª–µ–º:** 19

---

## üî¥ –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∏–π –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç (P0) - –ò—Å–ø—Ä–∞–≤–∏—Ç—å –Ω–µ–º–µ–¥–ª–µ–Ω–Ω–æ

### 1. –û—Ç—Å—É—Ç—Å—Ç–≤–∏–µ –ø—É–ª–∞ —Å–æ–µ–¥–∏–Ω–µ–Ω–∏–π Redis

**–§–∞–π–ª—ã:**
- `app/services/staging.py:20-22` (9+ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–π –≤ –º–µ—Ç–æ–¥–∞—Ö –Ω–∞ —Å—Ç—Ä–æ–∫–∞—Ö 84, 95, 106, 133, 162, 185, 204, 271, 323)
- `app/integrations/telegram/storage.py:40`

**–ü—Ä–æ–±–ª–µ–º–∞:**
```python
async def _get_redis(self):
    return await aioredis.from_url(self.redis_url, ...)  # –ù–æ–≤–æ–µ —Å–æ–µ–¥–∏–Ω–µ–Ω–∏–µ –∫–∞–∂–¥—ã–π —Ä–∞–∑!
```
–°–æ–∑–¥–∞–µ—Ç—Å—è –∏ –∑–∞–∫—Ä—ã–≤–∞–µ—Ç—Å—è –Ω–æ–≤–æ–µ —Å–æ–µ–¥–∏–Ω–µ–Ω–∏–µ –ø—Ä–∏ –∫–∞–∂–¥–æ–π –æ–ø–µ—Ä–∞—Ü–∏–∏.

**–í–ª–∏—è–Ω–∏–µ:**
- 9+ —Ü–∏–∫–ª–æ–≤ —Å–æ–∑–¥–∞–Ω–∏—è/–∑–∞–∫—Ä—ã—Ç–∏—è —Å–æ–µ–¥–∏–Ω–µ–Ω–∏—è –Ω–∞ –æ–ø–µ—Ä–∞—Ü–∏—é —Å —á–µ—Ä–Ω–æ–≤–∏–∫–æ–º
- –û–≥—Ä–æ–º–Ω—ã–µ –Ω–∞–∫–ª–∞–¥–Ω—ã–µ —Ä–∞—Å—Ö–æ–¥—ã –Ω–∞ —É—Å—Ç–∞–Ω–æ–≤–∫—É TCP —Å–æ–µ–¥–∏–Ω–µ–Ω–∏—è
- –ü–æ—Ç–µ–Ω—Ü–∏–∞–ª—å–Ω–æ–µ –∏—Å—á–µ—Ä–ø–∞–Ω–∏–µ file descriptors

**–ü–ª–∞–Ω –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏—è:**
```python
# –†–µ–∞–ª–∏–∑–æ–≤–∞—Ç—å singleton –ø–∞—Ç—Ç–µ—Ä–Ω –¥–ª—è –ø—É–ª–∞ —Å–æ–µ–¥–∏–Ω–µ–Ω–∏–π
class RedisPool:
    _instance = None
    _pool = None

    @classmethod
    async def get_pool(cls):
        if cls._pool is None:
            cls._pool = await aioredis.from_url(
                url,
                encoding="utf-8",
                decode_responses=True,
                max_connections=10
            )
        return cls._pool
```

**–û—Ü–µ–Ω–∫–∞ —É–ª—É—á—à–µ–Ω–∏—è:** 80-90% —Å–Ω–∏–∂–µ–Ω–∏–µ –≤—Ä–µ–º–µ–Ω–∏ –Ω–∞ –æ–ø–µ—Ä–∞—Ü–∏–∏ —Å Redis

---

### 2. N+1 –ø–∞—Ç—Ç–µ—Ä–Ω –≤ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏–∏ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤

**–§–∞–π–ª:** `app/services/qa_validators/duplicate_detector.py:29-60`

**–ü—Ä–æ–±–ª–µ–º–∞:**
```python
# –ú–µ—Ç–æ–¥ find_duplicates - —Å—Ç—Ä–æ–∫–∏ 29-34
for i in range(len(pairs)):           # O(n)
    for j in range(i + 1, len(pairs)):  # O(n)
        if cls._are_duplicate(pairs[i], pairs[j]):  # O(m)

# –ú–µ—Ç–æ–¥ remove_duplicates - —Å—Ç—Ä–æ–∫–∏ 54-60
for pair in pairs:
    for seen_q, idx in seen_questions.items():  # O(n¬≤)
        if cls._are_questions_similar(q_normalized, seen_q):  # difflib.SequenceMatcher - –¥–æ—Ä–æ–≥–æ
```

**–í–ª–∏—è–Ω–∏–µ:**
- –î–ª—è 1000 –ø–∞—Ä: ~500,000 —Å—Ä–∞–≤–Ω–µ–Ω–∏–π
- –ö–∞–∂–¥–æ–µ —Å—Ä–∞–≤–Ω–µ–Ω–∏–µ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç `difflib.SequenceMatcher()` - O(m*n)
- –≠–∫—Å–ø–æ–Ω–µ–Ω—Ü–∏–∞–ª—å–Ω—ã–π —Ä–æ—Å—Ç –≤—Ä–µ–º–µ–Ω–∏ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è

**–ü–ª–∞–Ω –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏—è:**

**–í–∞—Ä–∏–∞–Ω—Ç 1: –•—ç—à-–ø–æ–¥—Ö–æ–¥**
```python
from typing import Dict, Set

def find_duplicates_optimized(pairs: List[QAPair]) -> List[List[int]]:
    # –°–æ–∑–¥–∞—Ç—å –Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã–µ —Ö—ç—à–∏ –≤–æ–ø—Ä–æ—Å–æ–≤
    question_hashes: Dict[str, List[int]] = {}
    for i, pair in enumerate(pairs):
        normalized = normalize_text(pair.question)
        hash_key = " ".join(sorted(normalized.split()[:10]))  # –ü–µ—Ä–≤—ã–µ 10 —Å–ª–æ–≤
        question_hashes.setdefault(hash_key, []).append(i)

    # –ì—Ä—É–ø–ø—ã —Å –æ–¥–∏–Ω–∞–∫–æ–≤—ã–º–∏ —Ö—ç—à–∞–º–∏ - –∫–∞–Ω–¥–∏–¥–∞—Ç—ã –Ω–∞ –¥—É–±–ª–∏–∫–∞—Ç—ã
    duplicates = []
    for indices in question_hashes.values():
        if len(indices) > 1:
            # –¢–æ—á–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ —Ç–æ–ª—å–∫–æ –≤–Ω—É—Ç—Ä–∏ –≥—Ä—É–ø–ø—ã
            duplicates.extend(check_group_similarity(pairs, indices))

    return duplicates
```

**–í–∞—Ä–∏–∞–Ω—Ç 2: MinHash / LSH –¥–ª—è –ø—Ä–∏–±–ª–∏–∑–∏—Ç–µ–ª—å–Ω–æ–≥–æ –ø–æ–∏—Å–∫–∞**
```python
from datasketch import MinHash, MinHashLSH

def find_duplicates_minhash(pairs: List[QAPair], threshold=0.8):
    lsh = MinHashLSH(threshold=threshold, num_perm=128)
    minhashes = {}

    for i, pair in enumerate(pairs):
        m = MinHash(num_perm=128)
        for word in normalize_text(pair.question).split():
            m.update(word.encode('utf8'))
        lsh.insert(i, m)
        minhashes[i] = m

    duplicates = []
    processed = set()

    for i in range(len(pairs)):
        if i in processed:
            continue
        result = lsh.query(minhashes[i])
        if len(result) > 1:
            duplicates.append(list(result))
            processed.update(result)

    return duplicates
```

**–û—Ü–µ–Ω–∫–∞ —É–ª—É—á—à–µ–Ω–∏—è:**
- –í–∞—Ä–∏–∞–Ω—Ç 1: O(n¬≤) ‚Üí O(n*k) –≥–¥–µ k << n (—Ä–∞–∑–º–µ—Ä –≥—Ä—É–ø–ø—ã)
- –í–∞—Ä–∏–∞–Ω—Ç 2: O(n¬≤) ‚Üí O(n log n)
- –û–∂–∏–¥–∞–µ–º–æ–µ —É—Å–∫–æ—Ä–µ–Ω–∏–µ: 100-1000x –¥–ª—è –±–æ–ª—å—à–∏—Ö –¥–∞—Ç–∞—Å–µ—Ç–æ–≤

---

## ‚ö†Ô∏è –í—ã—Å–æ–∫–∏–π –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç (P1) - –ò—Å–ø—Ä–∞–≤–∏—Ç—å –≤ —Ç–µ—á–µ–Ω–∏–µ –Ω–µ–¥–µ–ª–∏

### 3. –õ–∏–Ω–µ–π–Ω—ã–π –ø–æ–∏—Å–∫ –≤ —Å–ø–∏—Å–∫–µ –∞–∫—Ç–∏–≤–Ω—ã—Ö —É–∑–ª–æ–≤

**–§–∞–π–ª:** `app/pipeline/graph_builder.py:81-251`

**–ü—Ä–æ–±–ª–µ–º–∞:**
```python
# 11+ –ø—Ä–æ–≤–µ—Ä–æ–∫ –≤–∏–¥–∞:
input_guardrails_enabled = "input_guardrails" in active_node_names  # O(n) –∫–∞–∂–¥—ã–π —Ä–∞–∑
if "session_starter" in active_node_names:  # O(n)
if "clarification_questions" in active_node_names:  # O(n)
# ... –∏ —Ç–∞–∫ –¥–∞–ª–µ–µ
```

**–í–ª–∏—è–Ω–∏–µ:**
- ~20 —É–∑–ª–æ–≤ –≤ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ √ó 11 –ø—Ä–æ–≤–µ—Ä–æ–∫ = 220 –∏—Ç–µ—Ä–∞—Ü–∏–π –ø–æ —Å–ø–∏—Å–∫—É
- –í—ã–ø–æ–ª–Ω—è–µ—Ç—Å—è –ø—Ä–∏ –∫–∞–∂–¥–æ–º –ø–æ—Å—Ç—Ä–æ–µ–Ω–∏–∏ –≥—Ä–∞—Ñ–∞

**–ü–ª–∞–Ω –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏—è:**
```python
# –°—Ç—Ä–æ–∫–∞ 55-58: –ø—Ä–∏ —Å–æ–∑–¥–∞–Ω–∏–∏ active_node_names
active_node_names = [n["name"] for n in pipeline_config if n.get("enabled", False)]
active_node_names_set = set(active_node_names)  # –î–û–ë–ê–í–ò–¢–¨ –≠–¢–£ –°–¢–†–û–ö–£

# –ó–∞—Ç–µ–º –≤–æ –≤—Å–µ—Ö –ø—Ä–æ–≤–µ—Ä–∫–∞—Ö –∑–∞–º–µ–Ω–∏—Ç—å:
# –ë–´–õ–û: "input_guardrails" in active_node_names
# –°–¢–ê–õ–û: "input_guardrails" in active_node_names_set

# –°—Ç—Ä–æ–∫–∏ –¥–ª—è –∑–∞–º–µ–Ω—ã: 81, 84, 109, 118, 124, 164, 175, 189-191, 203, 207, 217, 240, 251
```

**–û—Ü–µ–Ω–∫–∞ —É–ª—É—á—à–µ–Ω–∏—è:** O(n) ‚Üí O(1), —É—Å–∫–æ—Ä–µ–Ω–∏–µ ~20x –Ω–∞ –∫–∞–∂–¥—É—é –ø—Ä–æ–≤–µ—Ä–∫—É

---

### 4. –î–æ—Ä–æ–≥–∞—è —Å–µ—Ä–∏–∞–ª–∏–∑–∞—Ü–∏—è JSON –¥–ª—è –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è

**–§–∞–π–ª—ã:**
- `app/observability/input_state_filter.py:118-119`
- `app/observability/output_state_validator.py:135`
- `app/services/staging.py:305`

**–ü—Ä–æ–±–ª–µ–º–∞:**
```python
# –°–µ—Ä–∏–∞–ª–∏–∑–∞—Ü–∏—è –±–æ–ª—å—à–∏—Ö –æ–±—ä–µ–∫—Ç–æ–≤ —Ç–æ–ª—å–∫–æ –¥–ª—è –ø–æ–¥—Å—á–µ—Ç–∞ —Ä–∞–∑–º–µ—Ä–∞
original_size = len(json.dumps({k: state[k] for k in all_keys}, default=str))
filtered_size = len(json.dumps({k: state[k] for k in kept_keys}, default=str))
```

**–í–ª–∏—è–Ω–∏–µ:**
- –î–≤–æ–π–Ω–∞—è —Å–µ—Ä–∏–∞–ª–∏–∑–∞—Ü–∏—è –±–æ–ª—å—à–∏—Ö state –æ–±—ä–µ–∫—Ç–æ–≤
- –í—ã–ø–æ–ª–Ω—è–µ—Ç—Å—è –Ω–∞ –∫–∞–∂–¥–æ–º –∑–∞–ø—Ä–æ—Å–µ
- CPU –∏ –ø–∞–º—è—Ç—å —Ä–∞—Å—Ö–æ–¥—É—é—Ç—Å—è –Ω–∞ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É

**–ü–ª–∞–Ω –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏—è:**

**–í–∞—Ä–∏–∞–Ω—Ç 1: –ü—Ä–∏–±–ª–∏–∑–∏—Ç–µ–ª—å–Ω–∞—è –æ—Ü–µ–Ω–∫–∞ —Ä–∞–∑–º–µ—Ä–∞**
```python
def estimate_size(obj: Any, visited: set = None) -> int:
    """–ë—ã—Å—Ç—Ä–∞—è –æ—Ü–µ–Ω–∫–∞ —Ä–∞–∑–º–µ—Ä–∞ –æ–±—ä–µ–∫—Ç–∞ –±–µ–∑ —Å–µ—Ä–∏–∞–ª–∏–∑–∞—Ü–∏–∏"""
    if visited is None:
        visited = set()

    obj_id = id(obj)
    if obj_id in visited:
        return 0
    visited.add(obj_id)

    if isinstance(obj, str):
        return len(obj)
    elif isinstance(obj, (int, float, bool, type(None))):
        return 8
    elif isinstance(obj, (list, tuple)):
        return sum(estimate_size(item, visited) for item in obj)
    elif isinstance(obj, dict):
        return sum(estimate_size(k, visited) + estimate_size(v, visited)
                   for k, v in obj.items())
    else:
        return len(str(obj))

# –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ:
original_size = estimate_size({k: state[k] for k in all_keys})
filtered_size = estimate_size({k: state[k] for k in kept_keys})
```

**–í–∞—Ä–∏–∞–Ω—Ç 2: –£—Å–ª–æ–≤–Ω–æ–µ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ**
```python
# –°—á–∏—Ç–∞—Ç—å —Ä–∞–∑–º–µ—Ä —Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ —É—Ä–æ–≤–µ–Ω—å –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è DEBUG
if logger.isEnabledFor(logging.DEBUG):
    original_size = len(json.dumps({k: state[k] for k in all_keys}, default=str))
    filtered_size = len(json.dumps({k: state[k] for k in kept_keys}, default=str))
    logger.debug(f"Size reduction: {original_size} ‚Üí {filtered_size}")
```

**–û—Ü–µ–Ω–∫–∞ —É–ª—É—á—à–µ–Ω–∏—è:** 90% —Å–Ω–∏–∂–µ–Ω–∏–µ CPU –Ω–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ

---

### 5. –î–≤–æ–π–Ω–æ–π round-trip –≤ –∫—ç—à –ø—Ä–∏ –∫–∞–∂–¥–æ–º –ø–æ–ø–∞–¥–∞–Ω–∏–∏

**–§–∞–π–ª:** `app/services/cache/manager.py:85-92`

**–ü—Ä–æ–±–ª–µ–º–∞:**
```python
if data:
    entry = CacheEntry.model_validate_json(data)
    entry.hit_count += 1
    await self.set(query_normalized, entry)  # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è –æ–ø–µ—Ä–∞—Ü–∏—è SET!
```

**–í–ª–∏—è–Ω–∏–µ:**
- –ö–∞–∂–¥–æ–µ –ø–æ–ø–∞–¥–∞–Ω–∏–µ –≤ –∫—ç—à = 2 –æ–ø–µ—Ä–∞—Ü–∏–∏ Redis (GET + SET)
- –£–¥–≤–æ–µ–Ω–∏–µ latency –Ω–∞ –∫—ç—à–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –∑–∞–ø—Ä–æ—Å—ã

**–ü–ª–∞–Ω –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏—è:**

**–í–∞—Ä–∏–∞–Ω—Ç 1: Redis HINCRBY**
```python
async def get(self, query: str) -> Optional[CacheEntry]:
    query_normalized = self._normalize_query(query)

    if self.redis.is_available():
        key = f"{self.cache_prefix}{query_normalized}"

        # –ü–æ–ª—É—á–∏—Ç—å –¥–∞–Ω–Ω—ã–µ
        data = await self.redis.get(key)
        if data:
            entry = CacheEntry.model_validate_json(data)

            # –ò–Ω–∫—Ä–µ–º–µ–Ω—Ç —Å—á–µ—Ç—á–∏–∫–∞ –±–µ–∑ –ø–æ–≤—Ç–æ—Ä–Ω–æ–π —Å–µ—Ä–∏–∞–ª–∏–∑–∞—Ü–∏–∏
            await self.redis.hincrby(f"{key}:stats", "hit_count", 1)

            # –û–±–Ω–æ–≤–∏—Ç—å TTL
            await self.redis.expire(key, self.cache_ttl_seconds)

            return entry
```

**–í–∞—Ä–∏–∞–Ω—Ç 2: –ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ–µ –æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏**
```python
# –ù–µ –±–ª–æ–∫–∏—Ä–æ–≤–∞—Ç—å GET —Ä–∞–¥–∏ –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏
async def get(self, query: str) -> Optional[CacheEntry]:
    # ... –ø–æ–ª—É—á–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö ...
    if data:
        entry = CacheEntry.model_validate_json(data)

        # –û–±–Ω–æ–≤–∏—Ç—å —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –∞—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ (fire-and-forget)
        asyncio.create_task(self._update_hit_stats(query_normalized))

        return entry

async def _update_hit_stats(self, query: str):
    """–§–æ–Ω–æ–≤–æ–µ –æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏"""
    try:
        await self.redis.hincrby(f"{self.cache_prefix}{query}:stats", "hit_count", 1)
    except Exception as e:
        logger.debug(f"Failed to update cache stats: {e}")
```

**–û—Ü–µ–Ω–∫–∞ —É–ª—É—á—à–µ–Ω–∏—è:** 50% —Å–Ω–∏–∂–µ–Ω–∏–µ latency –Ω–∞ –∫—ç—à–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –∑–∞–ø—Ä–æ—Å—ã

---

### 6. –õ–∏–Ω–µ–π–Ω—ã–π –ø–æ–∏—Å–∫ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö –≤ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞—Ö

**–§–∞–π–ª:** `app/nodes/retrieval/search.py:93`

**–ü—Ä–æ–±–ª–µ–º–∞:**
```python
best_doc_metadata = next(
    (r.metadata for r in unique_results if r.content == best_doc_content), {}
)
```

**–í–ª–∏—è–Ω–∏–µ:**
- O(n) –ø–æ–∏—Å–∫ –ø—Ä–∏ –∫–∞–∂–¥–æ–º –∏–∑–≤–ª–µ—á–µ–Ω–∏–∏
- –°—Ä–∞–≤–Ω–µ–Ω–∏–µ —Å—Ç—Ä–æ–∫ (content) –º–æ–∂–µ—Ç –±—ã—Ç—å –¥–æ—Ä–æ–≥–∏–º

**–ü–ª–∞–Ω –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏—è:**
```python
# –°–æ–∑–¥–∞—Ç—å —Å–ª–æ–≤–∞—Ä—å –¥–ª—è O(1) lookup
content_to_metadata = {r.content: r.metadata for r in unique_results}
best_doc_metadata = content_to_metadata.get(best_doc_content, {})

# –ò–ª–∏ –µ—â–µ –ª—É—á—à–µ - —Ö—Ä–∞–Ω–∏—Ç—å –≤–º–µ—Å—Ç–µ:
content_metadata_pairs = [(r.content, r.metadata) for r in unique_results]
best_doc_content, best_doc_metadata = content_metadata_pairs[0]
```

**–û—Ü–µ–Ω–∫–∞ —É–ª—É—á—à–µ–Ω–∏—è:** O(n) ‚Üí O(1)

---

## üìä –°—Ä–µ–¥–Ω–∏–π –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç (P2) - –ò—Å–ø—Ä–∞–≤–∏—Ç—å –≤ —Ç–µ—á–µ–Ω–∏–µ –º–µ—Å—è—Ü–∞

### 7. –ù–µ—ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã–µ –≤—ã—á–∏—Å–ª–µ–Ω–∏—è —Å—Ä–µ–¥–Ω–µ–≥–æ –∑–Ω–∞—á–µ–Ω–∏—è

**–§–∞–π–ª—ã:**
- `app/services/cache/stats.py:138, 142`
- `app/services/metadata_generation/embedding_classifier.py:167, 172`

**–ü—Ä–æ–±–ª–µ–º–∞:**
```python
avg_cached_time = sum(self.cached_response_times) / len(self.cached_response_times)
avg_full_time = sum(self.full_response_times) / len(self.full_response_times)
```

**–ü–ª–∞–Ω –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏—è:**
```python
# –í–∞—Ä–∏–∞–Ω—Ç 1: statistics.mean()
from statistics import mean, StatisticsError

try:
    avg_cached_time = mean(self.cached_response_times)
    avg_full_time = mean(self.full_response_times)
except StatisticsError:
    avg_cached_time = avg_full_time = 0

# –í–∞—Ä–∏–∞–Ω—Ç 2: numpy (–µ—Å–ª–∏ —É–∂–µ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è)
import numpy as np
avg_cached_time = np.mean(self.cached_response_times)
avg_full_time = np.mean(self.full_response_times)

# –í–∞—Ä–∏–∞–Ω—Ç 3: –∏–Ω–∫—Ä–µ–º–µ–Ω—Ç–∞–ª—å–Ω–æ–µ –≤—ã—á–∏—Å–ª–µ–Ω–∏–µ —Å—Ä–µ–¥–Ω–µ–≥–æ
# –ü—Ä–∏ –¥–æ–±–∞–≤–ª–µ–Ω–∏–∏ –∫–∞–∂–¥–æ–≥–æ –∑–Ω–∞—á–µ–Ω–∏—è:
self.cached_sum += response_time
self.cached_count += 1
# –ü—Ä–∏ –ø–æ–ª—É—á–µ–Ω–∏–∏ —Å—Ä–µ–¥–Ω–µ–≥–æ:
avg_cached_time = self.cached_sum / self.cached_count if self.cached_count > 0 else 0
```

**–û—Ü–µ–Ω–∫–∞ —É–ª—É—á—à–µ–Ω–∏—è:** –ú–∏–Ω–∏–º–∞–ª—å–Ω–æ–µ, –Ω–æ —á–∏—â–µ –∫–æ–¥

---

### 8. –ë–ª–æ–∫–∏—Ä—É—é—â–∏–π DNS lookup –≤ async –∫–æ–¥–µ

**–§–∞–π–ª:** `app/utils/url_security.py:103`

**–ü—Ä–æ–±–ª–µ–º–∞:**
```python
addr_info = socket.getaddrinfo(hostname, None)  # –ë–ª–æ–∫–∏—Ä—É–µ—Ç event loop!
```

**–ü–ª–∞–Ω –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏—è:**
```python
import asyncio

# –í–∞—Ä–∏–∞–Ω—Ç 1: asyncio.get_event_loop().getaddrinfo()
async def _resolve_hostname_async(hostname: str) -> List[str]:
    loop = asyncio.get_event_loop()
    try:
        addr_info = await loop.getaddrinfo(hostname, None)
        return [addr[4][0] for addr in addr_info]
    except Exception as e:
        logger.warning(f"DNS resolution failed: {e}")
        return []

# –í–∞—Ä–∏–∞–Ω—Ç 2: aiodns (–Ω—É–∂–Ω–∞ —É—Å—Ç–∞–Ω–æ–≤–∫–∞)
import aiodns
resolver = aiodns.DNSResolver()

async def _resolve_hostname_async(hostname: str) -> List[str]:
    try:
        result = await resolver.gethostbyname(hostname, socket.AF_INET)
        return result.addresses
    except aiodns.error.DNSError as e:
        logger.warning(f"DNS resolution failed: {e}")
        return []
```

**–û—Ü–µ–Ω–∫–∞ —É–ª—É—á—à–µ–Ω–∏—è:** –ù–µ –±–ª–æ–∫–∏—Ä—É–µ—Ç event loop

---

### 9. FIFO –∫—ç—à –≤–º–µ—Å—Ç–æ LRU

**–§–∞–π–ª:** `app/nodes/classification/classifier.py:11-34`

**–ü—Ä–æ–±–ª–µ–º–∞:**
```python
self._cache = {}
self._cache_size = 1000

def _add_to_cache(self, text: str, output: ClassificationOutput):
    if len(self._cache) >= self._cache_size:
        self._cache.pop(next(iter(self._cache)))  # FIFO –≤–º–µ—Å—Ç–æ LRU
```

**–ü–ª–∞–Ω –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏—è:**
```python
from functools import lru_cache
from typing import Tuple

# –í–∞—Ä–∏–∞–Ω—Ç 1: functools.lru_cache
@lru_cache(maxsize=1000)
def _classify_cached(self, text: str) -> Tuple:
    """–û–±–µ—Ä—Ç–∫–∞ –¥–ª—è –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏—è (tuple –¥–ª—è hashable)"""
    result = self._classify_internal(text)
    return (result.category, result.confidence, tuple(result.keywords))

# –í–∞—Ä–∏–∞–Ω—Ç 2: collections.OrderedDict (manual LRU)
from collections import OrderedDict

self._cache = OrderedDict()

def _add_to_cache(self, text: str, output: ClassificationOutput):
    if text in self._cache:
        self._cache.move_to_end(text)  # –û–±–Ω–æ–≤–∏—Ç—å –ø–æ–∑–∏—Ü–∏—é
    else:
        if len(self._cache) >= self._cache_size:
            self._cache.popitem(last=False)  # –£–¥–∞–ª–∏—Ç—å —Å–∞–º—ã–π —Å—Ç–∞—Ä—ã–π
        self._cache[text] = output

# –í–∞—Ä–∏–∞–Ω—Ç 3: cachetools.LRUCache
from cachetools import LRUCache
self._cache = LRUCache(maxsize=1000)
```

**–û—Ü–µ–Ω–∫–∞ —É–ª—É—á—à–µ–Ω–∏—è:** –õ—É—á—à–∏–π hit rate, –º–µ–Ω—å—à–µ –ø—Ä–æ–º–∞—Ö–æ–≤ –∫—ç—à–∞

---

### 10. O(n√óm) –ø—Ä–æ–≤–µ—Ä–∫–∞ —Ç–æ—á–∫–∏ –≤ –ø—Ä—è–º–æ—É–≥–æ–ª—å–Ω–∏–∫–µ

**–§–∞–π–ª:** `app/services/document_loaders/pdf_loader.py:89-92`

**–ü—Ä–æ–±–ª–µ–º–∞:**
```python
for word in words:  # N —Å–ª–æ–≤
    for (tx0, ttop, tx1, tbottom) in table_rects:  # M —Ç–∞–±–ª–∏—Ü
        if tx0 <= cx <= tx1 and ttop <= cy <= tbottom:
```

**–ü–ª–∞–Ω –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏—è:**
```python
# –í–∞—Ä–∏–∞–Ω—Ç 1: R-tree spatial index
from rtree import index

def not_inside_tables_optimized(words, table_rects):
    # –ü–æ—Å—Ç—Ä–æ–∏—Ç—å R-tree –¥–ª—è —Ç–∞–±–ª–∏—Ü
    idx = index.Index()
    for i, (tx0, ttop, tx1, tbottom) in enumerate(table_rects):
        idx.insert(i, (tx0, ttop, tx1, tbottom))

    # –§–∏–ª—å—Ç—Ä–æ–≤–∞—Ç—å —Å–ª–æ–≤–∞
    result = []
    for word in words:
        cx, cy = word_center(word)
        # –ë—ã—Å—Ç—Ä—ã–π –ø–æ–∏—Å–∫ –ø–µ—Ä–µ—Å–µ—á–µ–Ω–∏–π - O(log m)
        if not list(idx.intersection((cx, cy, cx, cy))):
            result.append(word)

    return result

# –í–∞—Ä–∏–∞–Ω—Ç 2: –ü—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–∞—è —Å–æ—Ä—Ç–∏—Ä–æ–≤–∫–∞ (–µ—Å–ª–∏ R-tree –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω)
def not_inside_tables_sorted(words, table_rects):
    if not table_rects:
        return words

    # –°–æ—Ä—Ç–∏—Ä–æ–≤–∞—Ç—å —Ç–∞–±–ª–∏—Ü—ã –ø–æ X
    sorted_tables = sorted(table_rects, key=lambda t: t[0])

    result = []
    for word in words:
        cx, cy = word_center(word)

        # –ë–∏–Ω–∞—Ä–Ω—ã–π –ø–æ–∏—Å–∫ –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª—å–Ω—ã—Ö —Ç–∞–±–ª–∏—Ü
        inside = False
        for tx0, ttop, tx1, tbottom in sorted_tables:
            if cx < tx0:  # –í—Å–µ —Å–ª–µ–¥—É—é—â–∏–µ —Ç–∞–±–ª–∏—Ü—ã —Ç–æ–∂–µ —Å–ø—Ä–∞–≤–∞
                break
            if tx0 <= cx <= tx1 and ttop <= cy <= tbottom:
                inside = True
                break

        if not inside:
            result.append(word)

    return result
```

**–û—Ü–µ–Ω–∫–∞ —É–ª—É—á—à–µ–Ω–∏—è:** O(n√óm) ‚Üí O(n log m)

---

### 11. –ù–µ—ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã–µ –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏—è list‚Üîset

**–§–∞–π–ª—ã:**
- `app/nodes/aggregation/lightweight.py:33, 100, 130`
- `app/nodes/query_expansion/expander.py:35`

**–ü—Ä–æ–±–ª–µ–º–∞:**
```python
return {k: list(set(v)) for k, v in entities.items() if v}  # –¢–µ—Ä—è–µ—Ç—Å—è –ø–æ—Ä—è–¥–æ–∫
extras = list(set(extras))
all_queries = list(set([question] + [q.strip() for q in expanded_queries if q.strip()]))
```

**–ü–ª–∞–Ω –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏—è:**
```python
# –í–∞—Ä–∏–∞–Ω—Ç 1: dict.fromkeys() –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –ø–æ—Ä—è–¥–∫–∞ (Python 3.7+)
return {k: list(dict.fromkeys(v)) for k, v in entities.items() if v}
extras = list(dict.fromkeys(extras))

# –í–∞—Ä–∏–∞–Ω—Ç 2: –ï—Å–ª–∏ –ø–æ—Ä—è–¥–æ–∫ –Ω–µ –≤–∞–∂–µ–Ω - –æ—Å—Ç–∞–≤–∏—Ç—å set
return {k: list(set(v)) for k, v in entities.items() if v}  # OK –µ—Å–ª–∏ –ø–æ—Ä—è–¥–æ–∫ –Ω–µ –≤–∞–∂–µ–Ω

# –î–ª—è query expansion - —Å–æ—Ö—Ä–∞–Ω–∏—Ç—å –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–π –∑–∞–ø—Ä–æ—Å –ø–µ—Ä–≤—ã–º:
all_queries = [question] + [q.strip() for q in expanded_queries
                            if q.strip() and q.strip() != question]
# –ò–ª–∏:
seen = {question}
all_queries = [question]
for q in expanded_queries:
    q = q.strip()
    if q and q not in seen:
        all_queries.append(q)
        seen.add(q)
```

**–û—Ü–µ–Ω–∫–∞ —É–ª—É—á—à–µ–Ω–∏—è:** –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –ø–æ—Ä—è–¥–∫–∞, –ø—Ä–µ–¥—Å–∫–∞–∑—É–µ–º–æ—Å—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤

---

## üìù –ù–∏–∑–∫–∏–π –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç (P3) - –¢–µ—Ö–Ω–∏—á–µ—Å–∫–∏–π –¥–æ–ª–≥

### 12. –ù–∞–∫–æ–ø–ª–µ–Ω–∏–µ –±–æ–ª—å—à–∏—Ö —Å–ø–∏—Å–∫–æ–≤ –≤ SCAN —Ü–∏–∫–ª–∞—Ö

**–§–∞–π–ª—ã:**
- `app/services/cache/manager.py:123-128`
- `app/services/staging.py:277-281, 328-332, 336-340`

**–ü—Ä–æ–±–ª–µ–º–∞:**
```python
while True:
    cursor, keys = await self.redis.scan(cursor, match=pattern)
    keys_to_delete.extend(new_keys)  # –†–∞—Å—Ç—É—â–∏–π —Å–ø–∏—Å–æ–∫
```

**–ü–ª–∞–Ω –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏—è:**
```python
# –û–±—Ä–∞–±–∞—Ç—ã–≤–∞—Ç—å –±–∞—Ç—á–∞–º–∏ –≤–º–µ—Å—Ç–æ –Ω–∞–∫–æ–ø–ª–µ–Ω–∏—è
cursor = 0
deleted_count = 0
while True:
    cursor, keys = await self.redis.scan(cursor, match=pattern, count=100)

    if keys:
        # –û–±—Ä–∞–±–æ—Ç–∞—Ç—å —Å—Ä–∞–∑—É –≤–º–µ—Å—Ç–æ –Ω–∞–∫–æ–ø–ª–µ–Ω–∏—è
        deleted_count += await self._delete_batch(keys)

    if cursor == 0:
        break
```

---

### 13. –ö–æ–º–ø–∏–ª—è—Ü–∏—è regex –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤ –≤ —Ü–∏–∫–ª–µ

**–§–∞–π–ª:** `app/utils/url_security.py:152-159`

**–ü—Ä–æ–±–ª–µ–º–∞:**
```python
for pattern in suspicious_patterns:
    if re.search(pattern, url):  # –ö–æ–º–ø–∏–ª—è—Ü–∏—è –Ω–∞ –∫–∞–∂–¥–æ–π –∏—Ç–µ—Ä–∞—Ü–∏–∏
```

**–ü–ª–∞–Ω –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏—è:**
```python
# –ù–∞ —É—Ä–æ–≤–Ω–µ –º–æ–¥—É–ª—è
SUSPICIOUS_PATTERNS_COMPILED = [
    re.compile(r"\.\.\/"),
    re.compile(r"file:\/\/"),
    # ... –æ—Å—Ç–∞–ª—å–Ω—ã–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã
]

# –í —Ñ—É–Ω–∫—Ü–∏–∏:
for pattern in SUSPICIOUS_PATTERNS_COMPILED:
    if pattern.search(url):
        return False
```

---

### 14. –ö–æ–Ω–∫–∞—Ç–µ–Ω–∞—Ü–∏—è —Å—Ç—Ä–æ–∫ –≤ —Ü–∏–∫–ª–µ

**–§–∞–π–ª:** `app/services/document_loaders/pdf_loader.py:144, 176`

**–ü—Ä–æ–±–ª–µ–º–∞:**
```python
page_blocks[-1].content += " " + line_text  # O(n¬≤)
```

**–ü–ª–∞–Ω –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏—è:**
```python
# –ù–∞–∫–æ–ø–∏—Ç—å –≤ —Å–ø–∏—Å–æ–∫, –∑–∞—Ç–µ–º join
text_parts = []
for line in lines:
    text_parts.append(line_text)
final_text = " ".join(text_parts)
```

---

### 15. –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ range(len()) –≤–º–µ—Å—Ç–æ enumerate

**–§–∞–π–ª—ã:**
- `app/pipeline/graph_builder.py:170`
- `app/services/qa_validators/duplicate_detector.py:29-30`

**–ü—Ä–æ–±–ª–µ–º–∞:**
```python
for i in range(len(pipeline_nodes) - 1):
    current_node = pipeline_nodes[i]
    next_node = pipeline_nodes[i+1]
```

**–ü–ª–∞–Ω –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏—è:**
```python
# –í–∞—Ä–∏–∞–Ω—Ç 1: zip
for current_node, next_node in zip(pipeline_nodes, pipeline_nodes[1:]):
    # –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å current_node –∏ next_node

# –í–∞—Ä–∏–∞–Ω—Ç 2: enumerate (–µ—Å–ª–∏ –Ω—É–∂–µ–Ω –∏–Ω–¥–µ–∫—Å)
for i, current_node in enumerate(pipeline_nodes[:-1]):
    next_node = pipeline_nodes[i+1]
```

---

### 16. –ú–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ –∏—Ç–µ—Ä–∞—Ü–∏–∏ –ø–æ –æ–¥–Ω–∏–º –¥–∞–Ω–Ω—ã–º

**–§–∞–π–ª:** `app/nodes/retrieval/node.py:114-118`

**–ü—Ä–æ–±–ª–µ–º–∞:**
```python
for results in all_results:
    for r in results:
        if r.content not in seen_contents:
            unique_results.append(r)

docs = [r.content for r in unique_results]  # –í—Ç–æ—Ä–∞—è –∏—Ç–µ—Ä–∞—Ü–∏—è
scores = [r.score for r in unique_results]  # –¢—Ä–µ—Ç—å—è –∏—Ç–µ—Ä–∞—Ü–∏—è
```

**–ü–ª–∞–Ω –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏—è:**
```python
# –û–¥–Ω–∞ –∏—Ç–µ—Ä–∞—Ü–∏—è —Å —Ä–∞—Å–ø–∞–∫–æ–≤–∫–æ–π
unique_results = []
seen_contents = set()

for results in all_results:
    for r in results:
        if r.content not in seen_contents:
            unique_results.append(r)
            seen_contents.add(r.content)

# –†–∞—Å–ø–∞–∫–æ–≤–∞—Ç—å –∑–∞ –æ–¥–∏–Ω –ø—Ä–æ—Ö–æ–¥
docs, scores, metadatas = zip(*[(r.content, r.score, r.metadata)
                                 for r in unique_results]) if unique_results else ([], [], [])
```

---

### 17. –û—Ç—Å—É—Ç—Å—Ç–≤–∏–µ pipeline –¥–ª—è Redis –æ–ø–µ—Ä–∞—Ü–∏–π

**–§–∞–π–ª:** `app/services/staging.py:345-351`

**–ü—Ä–æ–±–ª–µ–º–∞:**
```python
for i in range(0, len(keys_to_delete), batch_size):
    batch = keys_to_delete[i:i + batch_size]
    if batch:
        deleted_count += await redis.delete(*batch)  # –û—Ç–¥–µ–ª—å–Ω—ã–π round-trip –Ω–∞ –±–∞—Ç—á
```

**–ü–ª–∞–Ω –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏—è:**
```python
# –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å Redis pipeline
pipeline = redis.pipeline()

for i in range(0, len(keys_to_delete), batch_size):
    batch = keys_to_delete[i:i + batch_size]
    if batch:
        pipeline.delete(*batch)

# –û–¥–∏–Ω round-trip –¥–ª—è –≤—Å–µ—Ö –æ–ø–µ—Ä–∞—Ü–∏–π
results = await pipeline.execute()
deleted_count = sum(results)
```

---

### 18. –ü–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω—ã–π –ø–∞—Ä—Å–∏–Ω–≥ JSON –≤ —Ü–∏–∫–ª–µ

**–§–∞–π–ª:** `app/services/staging.py:302-312`

**–ü—Ä–æ–±–ª–µ–º–∞:**
```python
for d_json in drafts_json:
    if d_json:
        d = json.loads(d_json)  # –ü–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ
```

**–ü–ª–∞–Ω –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏—è:**
```python
# –ï—Å–ª–∏ –¥–∞—Ç–∞—Å–µ—Ç –±–æ–ª—å—à–æ–π - –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å executor pool
import asyncio
from concurrent.futures import ThreadPoolExecutor

async def parse_drafts_parallel(drafts_json):
    loop = asyncio.get_event_loop()
    with ThreadPoolExecutor() as executor:
        tasks = [
            loop.run_in_executor(executor, json.loads, d_json)
            for d_json in drafts_json if d_json
        ]
        return await asyncio.gather(*tasks)

# –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ:
parsed_drafts = await parse_drafts_parallel(drafts_json)
```

---

### 19. –°–ª–æ–∂–Ω—ã–µ –≤–ª–æ–∂–µ–Ω–Ω—ã–µ list comprehensions

**–§–∞–π–ª:** `app/services/document_loaders/pdf_loader.py:58`

**–ü—Ä–æ–±–ª–µ–º–∞:**
```python
clean_table = [[(cell or "").strip() for cell in row] for row in table_data]
```

**–ü–ª–∞–Ω –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏—è:**
```python
# –î–ª—è –±–æ–ª—å—à–∏—Ö —Ç–∞–±–ª–∏—Ü - –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –≥–µ–Ω–µ—Ä–∞—Ç–æ—Ä –∏–ª–∏ numpy
def clean_table_data(table_data):
    for row in table_data:
        yield [cell.strip() if cell else "" for cell in row]

clean_table = list(clean_table_data(table_data))

# –ò–ª–∏ —Å numpy (–µ—Å–ª–∏ —Ç–∞–±–ª–∏—Ü—ã –±–æ–ª—å—à–∏–µ):
import numpy as np
table_array = np.array(table_data, dtype=str)
clean_table = np.char.strip(np.where(table_array == None, '', table_array)).tolist()
```

---

## üìà –°–≤–æ–¥–Ω–∞—è —Ç–∞–±–ª–∏—Ü–∞ –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–æ–≤

| –ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç | –ü—Ä–æ–±–ª–µ–º–∞ | –§–∞–π–ª | –û–∂–∏–¥–∞–µ–º–æ–µ —É–ª—É—á—à–µ–Ω–∏–µ |
|-----------|----------|------|---------------------|
| P0 | Redis connection pool | staging.py, telegram/storage.py | 80-90% |
| P0 | N+1 –≤ –¥–µ—Ç–µ–∫—Ç–æ—Ä–µ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤ | duplicate_detector.py | 100-1000x |
| P1 | –õ–∏–Ω–µ–π–Ω—ã–π –ø–æ–∏—Å–∫ –≤ —Å–ø–∏—Å–∫–µ | graph_builder.py | 20x |
| P1 | JSON serialization –¥–ª—è –ª–æ–≥–æ–≤ | input_state_filter.py | 90% |
| P1 | –î–≤–æ–π–Ω–æ–π cache round-trip | cache/manager.py | 50% |
| P1 | –õ–∏–Ω–µ–π–Ω—ã–π –ø–æ–∏—Å–∫ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö | retrieval/search.py | n‚Üí1 |
| P2 | –ë–ª–æ–∫–∏—Ä—É—é—â–∏–π DNS | url_security.py | –ù–µ –±–ª–æ–∫–∏—Ä—É–µ—Ç event loop |
| P2 | FIFO ‚Üí LRU –∫—ç—à | classifier.py | –õ—É—á—à–∏–π hit rate |
| P2 | O(n√óm) —Ç–æ—á–∫–∞ –≤ –ø—Ä—è–º–æ—É–≥–æ–ª—å–Ω–∏–∫–µ | pdf_loader.py | log(m) |
| P2 | –í—ã—á–∏—Å–ª–µ–Ω–∏–µ —Å—Ä–µ–¥–Ω–µ–≥–æ | stats.py | –ú–∏–Ω–∏–º–∞–ª—å–Ω–æ–µ |
| P2 | list‚Üîset –∫–æ–Ω–≤–µ—Ä—Å–∏–∏ | aggregation/lightweight.py | –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –ø–æ—Ä—è–¥–∫–∞ |
| P3 | SCAN —Å–ø–∏—Å–æ–∫ –∞–∫–∫—É–º—É–ª—è—Ü–∏—è | cache/manager.py | –ü–∞–º—è—Ç—å |
| P3 | Regex –∫–æ–º–ø–∏–ª—è—Ü–∏—è | url_security.py | CPU |
| P3 | –ö–æ–Ω–∫–∞—Ç–µ–Ω–∞—Ü–∏—è —Å—Ç—Ä–æ–∫ | pdf_loader.py | CPU |
| P3 | range(len()) | graph_builder.py | –ß–∏—Ç–∞–µ–º–æ—Å—Ç—å |
| P3 | –ú–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ –∏—Ç–µ—Ä–∞—Ü–∏–∏ | retrieval/node.py | CPU |
| P3 | Redis –±–µ–∑ pipeline | staging.py | Latency |
| P3 | –ü–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω—ã–π JSON parse | staging.py | –ü–∞—Ä–∞–ª–ª–µ–ª–∏–∑–º |
| P3 | –°–ª–æ–∂–Ω—ã–µ comprehensions | pdf_loader.py | –ß–∏—Ç–∞–µ–º–æ—Å—Ç—å |

---

## üéØ –†–µ–∫–æ–º–µ–Ω–¥—É–µ–º—ã–π –ø–æ—Ä—è–¥–æ–∫ –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏–π

1. **–ù–µ–¥–µ–ª—è 1:** P0 –ø—Ä–æ–±–ª–µ–º—ã (Redis pool + N+1)
2. **–ù–µ–¥–µ–ª—è 2:** P1 –ø—Ä–æ–±–ª–µ–º—ã (list lookups, JSON serialization, cache)
3. **–ù–µ–¥–µ–ª—è 3:** P2 –ø—Ä–æ–±–ª–µ–º—ã (DNS, LRU, O(n√óm))
4. **–ù–µ–¥–µ–ª—è 4:** P3 –ø—Ä–æ–±–ª–µ–º—ã (—Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏–π –¥–æ–ª–≥)

---

## üìä –û–∂–∏–¥–∞–µ–º—ã–π —ç—Ñ—Ñ–µ–∫—Ç

–ü–æ—Å–ª–µ –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏—è –≤—Å–µ—Ö P0 –∏ P1 –ø—Ä–æ–±–ª–µ–º:
- **Latency:** —Å–Ω–∏–∂–µ–Ω–∏–µ –Ω–∞ 60-80%
- **Throughput:** —É–≤–µ–ª–∏—á–µ–Ω–∏–µ –Ω–∞ 2-3x
- **CPU usage:** —Å–Ω–∏–∂–µ–Ω–∏–µ –Ω–∞ 40-50%
- **Memory usage:** —Å–Ω–∏–∂–µ–Ω–∏–µ –Ω–∞ 20-30%
- **Redis connections:** —Å–Ω–∏–∂–µ–Ω–∏–µ —Å ~100/sec –¥–æ ~1 –ø–æ—Å—Ç–æ—è–Ω–Ω–æ–µ —Å–æ–µ–¥–∏–Ω–µ–Ω–∏–µ

---

**–ü—Ä–∏–º–µ—á–∞–Ω–∏–µ:** –í—Å–µ –∏–∑–º–µ–Ω–µ–Ω–∏—è –¥–æ–ª–∂–Ω—ã –±—ã—Ç—å –ø–æ–∫—Ä—ã—Ç—ã —Ç–µ—Å—Ç–∞–º–∏ –ø–µ—Ä–µ–¥ –¥–µ–ø–ª–æ–µ–º –≤ production.
